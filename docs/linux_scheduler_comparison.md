# Unterschiede der Scheduler CFS, O(1), Brain Fuck Scheduler und SCHED_DEADLINE

Die dem Betriebssystem zur Verfügung stehende Rechenzeit muss an eine Vielzahl gleichzeitig aktiver Aktivitäten (Prozesse und Threads) verteilt werden. Moderne Rechnersysteme verfügen dabei über mehrere CPUs oder Prozessorkerne, deren Rechenleistung effizient und zielgerichtet genutzt werden muss. Für die Zuteilung der CPU-Zeit existieren unterschiedliche Vergabestrategien, die sich hinsichtlich Fairness, Skalierbarkeit und 
zeitlicher Garantien unterscheiden [1].

Im Kontext von Betriebssystemen wird diese Aufgabe von sogenannten Prozessscheduler übernommen [3]. Ein Scheduler ist eine zentrale Komponente des Betriebssystems, die die Abarbeitungsreihenfolge und Dauer der Bearbeitung von Prozessen organisiert [2]. Ziel des Scheduler ist es, seine verfügbaren Ressourcen möglichst effizient und angemessen auf alle laufenden Anwendungen zu verteilen [3].

Im folgenden Kapitel werden die Unterschiede zwischen den Linux-Scheduler CFS, O(1), Brain Fuck Scheduler und SCHED_DEADLINE herausgearbeitet. Als Grundlage für die nachfolgenden Ausführungen werden zunächst zentrale Begriffe eingeführt, die für das Verständnis der 
unterschiedlichen Scheduling-Ansätze erforderlich sind.

### Timesharing: 
Timesharing bezeichnet ein Scheduling-Prinzip, bei dem die CPU-Zeit unter mehreren lauffähigen Aktivitäten aufgeteilt wird. Jeder Prozess erhält für eine Zeitscheibe lang exklusiven Zugriff auf die CPU, bevor er unterbrochen und durch einen anderen Prozess ersetzt wird. Durch den schnellen Wechsel entsteht für den Benutzer der Eindruck paralleler Ausführung [7].

### Quantum (Zeitscheibe): 
Ein Quantum - auch Zeitscheibe genannt - ist die maximale Zeitspanne, die einem Prozess im Rahmen eines Timesharing-Scheduler am Stück CPU-Zeit zur Verfügung steht. Nach Ablauf dieses Zeitintervalls wird der Prozess unterbrochen und gegebenenfalls erneut in die Warteschlange eingeordnet [6].

### Priorität und Nice-Wert: 
Die Priorität eines Prozesses beschreibt dessen relative Bedeutsamkeit im Vergleich zu anderen Prozessen. Scheduler nutzen Prioritäten, um entscheiden zu können, welche Prozesse bevorzugt CPU-Zeit erhalten. Unter Linux wird die Priorität von Timesharing-Prozessen über den Nice-Wert beeinflusst. Er liegt typischerweise im Bereich von -20 bis +19, wobei ein niedrigerer Nice-Wert einer höheren Priorität entspricht [15].

## O(1) Scheduler
Seinen Namen verdankt der O(1) Scheduler der Eigenschaft, dass zentrale Scheduling-Operationen unabhängig von der Anzahl der zu verwaltenden Prozesse oder Threads mit konstanter Laufzeit O(1) ausgeführt werden. Die für eine Scheduling Entscheidung benötigte Prozessorzeit ist somit nicht von der Anzahl an Prozessen abhängig, was den Scheduler gut skalierbar und effizient macht [4]. Zyklisch bzw. beim Aufruf des Scheduler wird entschieden, welcher Prozess - oder in Multiprozessorsysteme welche Prozesse - als nächstes abgearbeitet werden. 

Ein zentrales Ziel des Schedulers ist eine gute Interaktivität des Systems. Prozesse, die häufig blockiert sind, daher die CPU ohnehin selten nutzen, wie beispielsweise ein- und ausgabeintensive Anwendungen, werden rechenintensiven Prozessen vorgezogen. Ob ein Prozess als interaktiv oder rechenintensiv einzustufen ist, wird vom Scheduler entschieden und erfordert komplexe Heuristiken und Algorithmen, ein großer Nachteil des O(1) Scheduler [8]. 
Grundsätzlich erhalten Timesharing-Prozesse mit höherer Priorität mehr CPU-Zeit zugewiesen (vgl. S. 129, [1]). Der Prioritätenbereich des Scheduler liegt auf einer Skala von 0 – 139, wobei 0 - 99 für Echtzeitprozesse reserviert ist und der Bereich 100 - 139 für zeitunkritische Timesharing-Prozesse genutzt wird. In beiden Fällen besitzt die kleinere Zahl eine höhere Priorität. 

Der O(1) Scheduler unterteilt sich in Hauptscheduler und eine periodisch aufgerufene Scheduling-Komponente. Letztere wird in festen Zeitabständen, sogenannten Ticks, aufgerufen und reduziert das Quantum der aktiven Prozesse pro Tick. Die Tick-Frequenz ist einstellbar und liegt typischerweise zwischen 100 Hz und 1000 Hz. Bei 1000 Hz ist das kleinstmögliche Intervall für gewährte Prozessorzeit also 1 ms. 20 Ticks werden als Standardwert für das Quantum verwendet. Der Hauptscheduler wird aufgerufen, wenn ein Prozess sein Quantum aufgebraucht hat oder ein aktiver Prozess blockiert. Dem Prozess wird die CPU entzogen so bald sein Quantum = 0 ist, der Prozess blockiert oder ein zuvor blockierter Prozess mit höherer Priorität bereit wird. 

Ist das Quantum einer Aktivität aufgebraucht wird es neu berechnet. Die Berechnung der Länge des Quantums hängt von der Priorität des Prozesses ab. Je höher die Priorität, desto größer ist das zugewiesene Quantum. Für eine Priorität von 100, was der höchsten Priorität von Timesharing Prozessen entspricht, wird ein Quantum von 800 ms ermittelt. Ein Quantum von 5 ms hingegen entspricht einer Priorität von 139. Neben der Priorität und dem Zeitintervall der gewährten CPU-Zeit - der Zeitscheibe - verwaltet der Scheduler auch für jeden Prozess einen Bonus, der zur Priorität addiert wird. Der Wertebereich dessen liegt zwischen -5 und 5. Prozesse mit hoher Inaktivität, etwa durch häufige Ein- und Ausgabeoperationen, erhalten einen negativen Bonuswert und werden somit belohnt. Je länger ein Prozess „schläft“, desto besser fällt der Bonus für ihn aus (S. 132ff, [1]). 
 
![Abb. 1: Run Queue des O(1) Scheduler, S. 133 [1]](images/Run_Queue_O(1)_Scheduler.png)

*Abbildung 1: Runqueue des O(1) Schedulers, S. 133 [1]*

Für jede CPU wird eine eigene Runqueue gepflegt. Sie enthält Zeiger auf die eigentlichen Prozesswarteschlangen. Der O(1) Scheduler organisiert die auszuführenden Aufgaben dabei in zwei Warteschlangen, eine „active“-Queue für Prozesse mit verbleibendem Quantum und eine „expired“-Queue für abgelaufene oder unterbrochene Prozesse [5]. Verbraucht ein Vorgang sein Quantum oder wird er unterbrochen, wird er in die expired Queue verschoben. Sobald alle Prozesse in dieser Queue stehen, werden die Quanten neu bestimmt und die Rollen der beiden Queues durch einen einfachen Zeigertausch gewechselt. Die Abarbeitung beginnt anschließend von Neuem.

Innerhalb der „active“- und „expired“-Queues werden die Prozesse zusätzlich nach Priorität sortiert. Für jede Prioritätsstufe existiert eine eigene Liste von Prozessen gleicher Priorität. Eine Bitmap zeigt mit einer gesetzten 1 an, welche dieser Listen nicht leer sind, damit der Scheduler den nächsten auszuführenden Prozess effizient bestimmen kann (S. 132 - 134, [1]). Der Scheduler war über mehrere Kernel-Versionen hinweg der Standard-Scheduler in Linux Systemen und eignet sich insbesondere für klassische Desktop- und Serverumgebungen mit vielen gleichzeitig aktiven Prozessen. Aufgrund seiner konstanten Laufzeit und guten Skalierbarkeit war er für die damaligen Hardwarearchitekturen gut geeignet ([2], [4]).

Trotz seiner Vorteile weist der O(1) Scheduler auch strukturelle Nachteile auf. Die Verwendung fester Zeitscheiben erschwert die Anpassung an dynamische oder stark wechselnde Workloads. Zudem nahm die Komplexität des Scheduler im Laufe der Zeit durch zusätzliche Heuristiken und Optimierungen stetig zu, was die Wartbarkeit erschwerte und das Verhalten zunehmend schwer vorhersehbar machte. Aufgrund seiner gewachsenen Komplexität wurde der O(1) Scheduler jedoch vom weitaus schlankeren Completely Fair Scheduler abgelöst [5].


## Completely Fair Scheduler (CFS)
Wie der Name es vermuten lässt, allokiert der Completely Fair Scheduler CPU-Ressourcen möglichst fair unter den ausgeführten Aufgaben, ohne dabei die Performanz des Rechners zu beeinträchtigen [2]. Auch der CFS unterstützt Timesharing – Aktivitäten. Die zugrundeliegende Idee basiert auf einer idealen Multitasking-CPU, die ihre Rechenleistung anteilig mit 1/N (wobei N die Anzahl der Prozesse ist) an jeden Prozess verteilt. Ziel ist es, diese ideale Verteilung möglichst genau nachzubilden. Im Gegensatz zu klassischen prioritätsbasierten Scheduler erfolgt beim CFS keine direkte Prioritätenberechnung. Alle Prozesse besitzen standardmäßig die gleiche Basispriorität (nice=0) (S. 134ff, [1]; [5]).

Im Unterschied zu seinem Vorgänger - der O(1) Scheduler - verwaltet der CFS keine Statistiken zu laufenden Vorgängen und organisiert sie in keiner Run Queue. Jeder Prozess erhält eine Variable „vruntime“, in der angegeben ist, wie viel Zeit die tatsächliche Nutzung der CPU von der idealen CPU-Verteilung (1/N) abweicht. Betrachtet wird dabei diejenige Zeit, die der Prozess auf eine CPU-Zuteilung wartet, sobald er sich im Zustand „bereit“ befindet. Hat ein Prozess in der Vergangenheit zu wenig Rechenzeit erhalten, besitzt er eine kleinere vruntime und wird in Zukunft bevorzug, solange bis sich seine „vruntime“ den anderen Prozessen annähert. Dann wird ihm die CPU wieder entzogen. Somit wird eine faire CPU-Verteilung erreicht (S. 134ff, [1]; [5]). 

Wie oben bereits angeschnitten ist auch die Struktur, in der die Vorgänge organisiert werden, anders als noch beim O(1) Scheduler. Jedem CPU-Kern ist ein eigener Completely Fair Scheduler zugeordnet, der die Prozesse jeweils in einem Rot-Schwarz-Baum (Red/Black-Tree) verwaltet. Die Einordnung in den Baum wird dabei auf Basis des vruntime-Wertes bestimmt (S. 134ff, [1]).
 
![Abb. 2: Red/Black-Tree des CFS, [5]](images/CFS_Red_Black_Tree.png) 

*Abbildung 2: Red/Black-Tree des CFS, [5]*

Der Rot-Schwarz-Baum ist ein ausgeglichener binärer Suchbaum (AVL-Baum), der eine logarithmische Laufzeit von O(log n) für Operationen wie Suchen, Einfügen und Löschen erlaubt, wobei n die Anzahl der aktuell vorhandenen Prozesse als Knoten darstellt. Vorgänge werden durch sched_entity Objekte repräsentiert. Ähnlich wie bei der Priorität des O(1) Scheduler sind die Prozesse mit der geringsten virtual runtime (vruntime) diejenigen, die den größten Bedarf an Rechenleistung besitzen und umgekehrt. Sie werden im Rot-Schwarz-Baum auf der linken bzw. rechten Seite einsortiert. Der Scheduler wählt als nächsten Prozess denjenigen aus, der ganz links außen steht. Während der Task läuft, wird die verbrauchte CPU-Zeit mit der vruntime addiert. Hat er seine Laufphase beendet, wird er wieder zurück in den Baum eingeordnet, jetzt wird er weiter rechts landen, weil seine vruntime gestiegen ist. Durch dieses Verhalten wandern die Prozesse von rechts nach links und zurück. Dadurch ergibt sich eine faire Aufteilung der Rechenzeit unter den Prozessen (S. 134ff, [1]; [5]).

Ein wesentlicher Vorteil des Completely Fair Scheduler liegt in seiner fairen und vorhersehbaren Verteilung der CPU-Zeit. Durch die Variable vruntime wird vermieden, dass einzelne Prozesse dauerhaft bevorzugt oder benachteiligt werden, wie es beim O(1) Scheduler noch der Fall ist. Zudem skaliert der CFS durch seine einzelnen Runqueues pro CPU-Kern sehr gut auf Mehrkernsystemen [14]. Darüber hinaus bietet er eine gute Interaktivität für typische Desktop- und Server Workloads. Prozesse, die häufig blockieren oder nur kurze Rechenphasen benötigen, wie Ein- bzw. Ausgaben, erhalten aufgrund ihrer kleinen vruntime bevorzugt CPU-Zeit, was zu kurzen Reaktionszeiten bei interaktiven Anwendungen führt [15]. Diese Eigenschaften machten ihn lange zum Standard-Scheduler für normale Prozesse im Linux-Kernel, wie auf Desktop-Systemen oder in Server-Umgebungen [14,15].

Auf der anderen Seite kann der Completely Fair Scheduler keine harten zeitlichen Garantien zu Deadlines für die Abarbeitung von Prozessen geben. Dadurch ist er in zeitkritischen Anwendungsgebieten - wie im Embedded Bereich - ungeeignet [14].

## Brain Fuck Scheduler
Der Brain Fuck Scheduler wurde als bewusst einfach gehaltener Gegenentwurf zum Completely Fair Scheduler entwickelt. Während CFS eine mathematisch fundierte Fairness durch die Verwaltung lauffähiger Prozesse in einem nach virtueller Laufzeit sortierten Rot-Schwarz-Baum anstrebt, verwendet der BFS – ähnlich wie der O(1) Scheduler – eine Runqueue. Allerding verzichtet er auf die Trennung in eine „active“ und eine „expired“ Queue [8, 9].

Im Gegensatz zu CFS, der für jede CPU einen eigenen Scheduler verwendet, werden beim BFS alle lauffähigen Prozesse in einer einzigen globalen doppelt verketteten Liste, die als run queue fungiert, verwaltet [1, 8]. Eine unter den CPUs geteilte Runqueue hat den Vorteil, dass Prozesse schnell von überlasteten auf weniger ausgelasteten CPU-Kernen verlagert werden können [8]. Dadurch wird eine niedrige Latenz und gute Performance bei kleinem bis mittlerem CPU-Count erzielt [10].

Das Einfügen eines Prozesses in diese Liste besitzt eine konstante Laufzeit von O(1), während die Auswahl des nächsten Prozesses jedoch im ungünstigsten Fall eine vollständige Durchsuchung der Liste erfordert und somit eine lineare Laufzeit von O(n) besitzt. Bei sehr vielen Vorgängen wirkt sich dies jedoch negativ auf die Laufzeit, des BFS aus [8, 9].

Jeder Prozess besitzt neben seiner Priorität eine Zeitscheibe und eine sogenannte „virtual deadline“. Diese beschreibt die maximale Zeit, die ein Task im Vergleich zu einem anderen Vorgang gleicher Priorität warten muss, bis er zum ersten Mal oder erneut CPU-Zeit erhält. Im Unterschied zur virtuellen Laufzeit des CFS stellt diese Deadline keinen kontinuierlichen Fairness-Wert dar. „Virtual“ verdeutlicht, dass keine zeitliche Garantie für die Ausführung besteht. Prozesse mit einer früheren virtuellen Deadline werden früher eingeplant als welche mit höherer.

Prozesse mit höherer Priorität erhalten entsprechend frühere virtuelle Deadlines, somit rücken sie in der Runqueue weiter nach vorne und erhalten damit schneller CPU-Zeit. Auf diese Weise entsteht Fairness innerhalb gleicher Prioritätsstufen sowie eine gezielte Bevorzugung höherer priorisierter Prozesse. Wird die Zeitscheibe eines Prozesses nach dem Aufruf vollständig aufgebraucht, wird er gemäß dem beschriebenen Verfahren erneut in die Runqueue einsortiert. Mit der Verwendung einer einzigen globalen Runqueue besitzt BFS im Vergleich zum CFS jedoch eine geringe Skalierbarkeit. Dadurch verschlechtert sich die Performance deutlich bei mehrkernigen oder parallelen Prozessen [11].

## SCHED_DEADLINE
Der SCHED_DEADLINE ist ein CPU Scheduler, der im Linux-Kernel seit Version 3.14 verfügbar ist und sich grundlegend von klassischen Timesharing-Schedulern wie CFS oder O(1) unterscheidet. Während letztere auf Fairness oder Interaktivität im allgemeinen Betrieb abzielen, ist SCHED_DEADLINE ein Scheduler, der Aufgaben mit harten zeitlichen Anforderungen unterstützt [12, 13]. Weil es sich um keinen Timesharing-Scheduler handelt, kann es dazu kommen, dass Prozesse mit höherer Priorität, die länger als geplant laufen, Vorgänge mit niedrigerer Priorität willkürlich und unkontrolliert verzögern [13]. SCHED_DEADLINE hat für dieses Problem folgenden Lösungsansatz.

Im Kern basiert SCHED_DEADLINE auf dem Earliest Deadline First (EDF)-Algorithmus, kombiniert mit dem Constant Bandwidth Server (CBS)-Mechanismus, um Prozesse voneinander zu isolieren. Im Gegensatz zu CFS, der primär Fairness über die virtuelle Laufzeit (vruntime) zwischen normal-priorisierten Prozessen herstellt, werden bei SCHED_DEADLINE explizite zeitliche Anforderungen berücksichtigt. Jeder Task erhält die Parameter „runtime“, „period“ und „deadline“. Alle n Zeiteinheiten („period“) erhält ein Prozess eine bestimmte CPU-Zeit („runtime“) in Mikrosekunden bevor eine Frist („deadline”) ebenfalls in Mikrosekunden abgelaufen ist. Dabei ist die „period“ der Mindestzeitabstand zwischen aufeinanderfolgenden Aktivierungen des Prozesses und „runtime“ die im schlimmsten Fall benötigte Laufzeit [12, 13].

Bei SCHED_DEADLINE wird für jeden Prozess individuell die benötigte CPU-Zeit bestimmt. Anschließend wählt der Scheduler unter allen Deadline-Tasks denjenigen mit der frühesten Deadline zur Ausführung aus – entsprechend dem EDF-Prinzip, bei dem Aufgaben mit enger bevorstehender Frist Vorrang haben. Der CBS-Mechanismus auf der anderen Seite hindert einen Prozess daran, mehr als sein vorgesehenes Budget zu verbrauchen, damit er auch keine anderen wartenden Prozesse behindern kann, was der Scheduler anderen Echtzeit Scheduler, wie z. B. SCHED_FIFIO, voraushat. Man spricht dabei von zeitlicher Isolation [12].

Während der Laufzeit des Prozesses reduziert sich die verbleibende Runtime mit jedem Takt. Erreicht diese Runtime Null, wird der Task sofort blockiert („throttled“) und kann erst nach Ablauf der „deadline“ erneut ausgeführt werden. Dadurch kann kein Prozess mehr CPU-Zeit als vereinbart verbrauchen. Gleichzeitig sorgt eine sog. Admission Control dafür, dass die Summe der benötigten CPU-Anteile der aktiven Deadline-Tasks geringer als 100 % ist. Jedoch können durch diesen Mechanismus auch neue Prozesse abgelehnt werden, wenn das System bereits ausgelastet ist. Ferner ist eine exakte Kenntnis der zeitlichen Anforderungen eines Vorgangs nötig. Jedoch wird aber mit der Admission Control sichergestellt, dass das System im realen Betrieb nicht überlastet wird und zeitliche Zusagen garantiert werden können, was ein großer Vorteil für den Scheduler ist [12, 13]. Deswegen eignet er sich auch für Echtzeitanwendungen wie Multimedia-Verarbeitung, industrielle Steuerungs- und Regelungssysteme sowie eingebettete Systeme mit klar definierten zeitlichen Anforderungen [12].

Im Vergleich zum CFS bietet SCHED_DEADLINE damit härtere zeitliche Garantien, da Tasks mit realen Deadlines geplant werden und ihr zeitlicher Bedarf pro Periode eingehalten werden soll. Damit ist er eine gute Ergänzung für zeitkritische Anwendungen. CFS hingegen optimiert auf faire Verteilung der CPU-Ressource ohne expliziten Deadline-Bedingungen.

## Fazit
Die Analyse der verschiedenen Linux-Scheduler zeigt, dass sich die betrachteten Scheduling-Strategien deutlich in ihren Zielsetzungen und Einsatzbereichen unterscheiden. Der O(1) Scheduler verfolgte das Ziel konstanter Laufzeiten und guter Skalierbarkeit, erreichte diese Eigenschaften jedoch nur auf Kosten wachsender Komplexität und eingeschränkter Fairness. Der Completely Fair Scheduler stellt einen konzeptionell einfacheren und ausgewogeneren Ansatz dar, indem er CPU-Zeit fair zwischen Prozessen verteilt. Der Brain Fuck Scheduler verfolgt einen bewusst vereinfachten Ansatz, verzichtet jedoch auf Skalierbarkeit und eignet sich daher vor allem für Systeme mit geringer Kernanzahl. SCHED_DEADLINE adressiert hingegen ein grundlegend anderes Problemfeld, indem er harte zeitliche Anforderungen unterstützt und durch deadline-basierte Planung sowie zeitlicher Isolation deterministische CPU-Zuteilung ermöglicht.

Insgesamt zeigt sich, dass kein Scheduler universell optimal ist. Vielmehr hängt die Wahl eines geeigneten Scheduler entscheidend von den jeweiligen Anforderungen an Fairness, Interaktivität, Skalierbarkeit und zeitlichen Garantien ab. Die Koexistenz unterschiedlicher Scheduling-Strategien im Linux-Kernel ermöglicht es, diese vielfältigen Anforderungen flexibel abzudecken.
